---
title: "Perceptron"
author: "Joyce Robbins"
date: "8/4/2018"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r}
# based on https://github.com/log0/perceptron/blob/master/perceptron.r
#
# Compile Report to see all interations
#
# Perceptron Learning Algorithm implementation
#
# This script demonstrates how to implement a working perceptron, one of the most basic
# algorithm in the machine learning field.
#
# As long as the data is linearly separable, it is guaranteed to converge in a finite
# number of iterations.

####################################################################################
# The core perceptron learning algorithm
#
# 1) Initialize the weight vector W to 0
# 2) Calculate h(x) = sign(transpose of W * (X))
# 3) Pick any misclassified point (xn, yn)
# 4) Update the weight vector by w <- w + yn * xn
# 5) Repeat until no points are misclassified
####################################################################################
perceptron <- function(X, Y)
{

  converged <- F

  # Initialize weight vector to 0
  W <- matrix(0, 1, 3)
  X <- cbind(rep(1, N), X)
  
  mag <- function(w) sqrt(w[1]^2+w[2]^2)

  par(pty="s")
  tmp <- vector()
  # Run for 10000 iterations
  for (i in 1:10000)
  {
# print info
    print(paste("Iteration", i))
    print("Weights:")
    print(W)
    
# determine decision boundary
    
    intercept <- -W[1]/W[3]
    slope <- -W[2]/W[3]
    print(paste("Slope: ", slope))
    print(paste("Intercept: ", intercept))

  
#    shift <- -W[1]/mag(W[,2:3])
#    print(paste("Shift: ", shift))
   
    tmp <- readline(paste("Iteration", i, " continue"))

#     if(!is.na(shift)) {    

    xshift <- intercept / (W[2]/W[3] + W[3]/W[2])
    yshift <- xshift * W[3]/W[2]
    
    print(paste("xshift: ", xshift))
    print(paste("yshift ", yshift))
# } else {
#   xshift <- 0
#   yshift <- 0
# }
    
    # Calculate h(x) with the weight vector W and the data input X
    h.X <- fn.sign(W %*% t(X))

    # Calculate the misclassified mask
    misclassified.mask <- h.X != Y

    # Check if all of the points are classified correctly
    if (sum(misclassified.mask) != 0) {
      # No! We have to update the weight vector now using any one of the misclassified input

      # Get the misclassified points out
      misclassified.points <- X[misclassified.mask, , drop = F]
      misclassified.points.Y <- Y[misclassified.mask]

      # Get one of them
      misclassified.point.index <- sample(dim(misclassified.points)[1], 1)
      misclassified.point <- misclassified.points[misclassified.point.index, , drop = F]
      misclassified.point.Y <- misclassified.points.Y[misclassified.point.index]
      
     # Now update the weight vector
      W <- W + misclassified.point.Y %*% misclassified.point
      
      print("misclassified.point:")
      print(misclassified.point)
      print("misclassified.point.Y:")
      print(misclassified.point.Y)
      print("m.p.y %*% m.p:")
      print(misclassified.point.Y %*% misclassified.point)
      
# plot stuff      
    plot(X[,2:3], pch = 16, col = as.factor(Y))
    abline(a = intercept, b = slope, col="red")
    arrows(xshift, yshift, W[2] + xshift ,
           W[3] + yshift, col = "green")
      points(misclassified.point[1, 2],
             misclassified.point[1, 3],
             col = "green", cex = 2)
       arrows(xshift, yshift,
              W[2]+xshift+misclassified.point[1,2],
              W[3]+yshift+misclassified.point[1,3],
              col = "lightblue")

    } else {
      # Yes! We are done.
      converged <- T
      break
    }

    # repeat
  }

  if (converged)
  {
    cat('Converged! Iteration ', i, ' , with final weight : ', W, '\n')
    # plot stuff      
    plot(X[,2:3], pch = 16, col = as.factor(Y))
    lines(db, col="red")
    arrows(xshift, yshift, W[2]+xshift,
           W[3]+yshift, col = "green")
      points(misclassified.point[1, 2],
             misclassified.point[1, 3],
             col = "green", cex = 2)
      arrows(xshift, yshift,
             W[2]+xshift+misclassified.point[1,2],
             W[3]+yshift+misclassified.point[1,3],
             col = "lightblue")

    
  }
  else
  {
    cat('DID NOT CONVERGE!\n')
  }

  return(W)
}


#####################\###############################################################
# The SIGN function in the perceptron learning algorithm
####################################################################################
fn.sign <- function(values)
{
  return(ifelse(values > 0, 1, -1))
}

####################################################################################
# This function calculates the two ending points of the decision boundary given
# a weight vector. Not necessarily in practice, but just to help visualize for
# you to see.
####################################################################################




# Define initial points

N <- 10
X <- matrix(c(-1, 1, 2, 4, 6, 4, 6, 6, 5, 8, -3, 7, 4, 4, 2, 8, 6, 8, 10, 10)/5, N)

Y <- c(-1, -1, -1, -1, -1, 1, 1, 1, 1, 1)

# 
# N <- 4
# X <- matrix(c(0,0,1,1,0,1,0,1), 4)
# Y <- c(-1, 1, 1, 1)

# Run perceptron algorithm
W <- perceptron(X, Y)

# Plot learned decision boundary
#lines(decision.boundary(W), col="red")

title(paste("Perceptron training against N = ", N))



```

